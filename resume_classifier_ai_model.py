# -*- coding: utf-8 -*-
"""RESUME CLASSIFIER AI MODEL

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e3csSp9kUZEPWDz1Xf1T6u3VQaaCw6YX
"""

!pip install transformers datasets torch scikit-learn pandas

import pandas as pd

file_path = '/content/resume_classifier_dataset.csv'
df = pd.read_csv(file_path)

print(df.head())
print(df['category'].value_counts())

from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()
df["label"] = label_encoder.fit_transform(df["category"])

num_labels = len(label_encoder.classes_)
print("Number of classes:", num_labels)
print(label_encoder.classes_)

label_mapping = dict(zip(label_encoder.classes_, range(num_labels)))
print(label_mapping)

from sklearn.model_selection import train_test_split

train_df, val_df = train_test_split(
    df,
    test_size=0.2,
    random_state=42,
    stratify=df["label"]
)

train_df = train_df.reset_index(drop=True)
val_df = val_df.reset_index(drop=True)

from datasets import Dataset

train_dataset = Dataset.from_pandas(train_df)
val_dataset = Dataset.from_pandas(val_df)

from transformers import AutoTokenizer, AutoModelForSequenceClassification

model_name = "distilbert-base-uncased"

tokenizer = AutoTokenizer.from_pretrained(model_name)

model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=num_labels
)

def tokenize_function(example):
    return tokenizer(
        example["resume_text"],
        padding=True,
        truncation=True,
        max_length=128
    )

train_dataset = train_dataset.map(tokenize_function)
val_dataset = val_dataset.map(tokenize_function)

train_dataset = train_dataset.rename_column("label", "labels")
val_dataset = val_dataset.rename_column("label", "labels")

train_dataset.set_format("torch", columns=["input_ids", "attention_mask", "labels"])
val_dataset.set_format("torch", columns=["input_ids", "attention_mask", "labels"])

from transformers import Trainer, TrainingArguments, IntervalStrategy

training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy=IntervalStrategy.EPOCH, # Renamed from evaluation_strategy
    save_strategy=IntervalStrategy.NO, # Renamed from save_strategy to align with common alternatives
    num_train_epochs=3,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    logging_steps=50,
    load_best_model_at_end=False
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset
)
print("Training arguments and Trainer successfully initialized.")

from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    data_collator=data_collator # Pass the data collator here
)

trainer.train()
print("Model training completed.")

eval_results = trainer.evaluate()
print("Evaluation Results:", eval_results)

model.save_pretrained("resume_classifier_model")
tokenizer.save_pretrained("resume_classifier_model")

from transformers import pipeline
import pickle

# Save label encoder
with open("label_encoder.pkl", "wb") as f:
    pickle.dump(label_encoder, f)

classifier = pipeline(
    "text-classification",
    model="resume_classifier_model",
    tokenizer="resume_classifier_model"
)

# Load label encoder
with open("label_encoder.pkl", "rb") as f:
    label_encoder = pickle.load(f)

text = """
Experienced Python developer with Machine Learning,
TensorFlow, and data analysis experience.
"""

prediction = classifier(text)[0]

predicted_label = int(prediction["label"].split("_")[-1])
predicted_category = label_encoder.inverse_transform([predicted_label])[0]

print("Predicted Category:", predicted_category)